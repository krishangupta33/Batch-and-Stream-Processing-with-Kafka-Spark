{"cells":[{"cell_type":"markdown","source":["# 1. Setup\nInstall Confluent-Kafka client"],"metadata":{}},{"cell_type":"code","source":["! pip show confluent-kafka"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["# 2. Batch Processing"],"metadata":{}},{"cell_type":"markdown","source":["## 2.1. Load topic data from Confluent in batch mode"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions\ntopic_name = \"databricks_test\"\nbootstrap.servers = \"provide your confluent endpoint\"\ndf_kafka = spark.read.format(\"kafka\")\\\n    .option(\"kafka.bootstrap.servers\", bootstrap.servers)\\\n    .option(\"subscribe\", topic_name)\\\n    .option(\"kafka.security.protocol\",\"SASL_SSL\")\\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"Replace with your api key\\\" password=\\\"replace with your api secret\\\";\")\\\n    .load()\n\ndisplay(df_kafka)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th><th>topic</th><th>partition</th><th>offset</th><th>timestamp</th><th>timestampType</th></tr></thead><tbody><tr><td>Mw==</td><td>eyJuYW1lIjogIkpvaG4gRG9lIiwgImFnZSI6IDM1LCAiZW1haWwiOiAiam9obi5kb2VAZ21haWwuY29tIn0=</td><td>databricks_test</td><td>0</td><td>0</td><td>2020-01-20T18:56:05.097+0000</td><td>0</td></tr><tr><td>MQ==</td><td>eyJuYW1lIjogIkphbmUgRG9lIiwgImFnZSI6IDI5LCAiZW1haWwiOiAiamFuZS5kb2VAZ21haWwuY29tIn0=</td><td>databricks_test</td><td>0</td><td>1</td><td>2020-01-20T18:56:05.097+0000</td><td>0</td></tr><tr><td>Mg==</td><td>eyJuYW1lIjogIkZhdGloIiwgImFnZSI6IDI0LCAiZW1haWwiOiAiZi5uYXllYmlAZ21haWwuY29tIn0=</td><td>databricks_test</td><td>0</td><td>2</td><td>2020-01-20T18:56:05.204+0000</td><td>0</td></tr><tr><td>Mg==</td><td>eyJuYW1lIjogIkZhdGloIiwgImFnZSI6IDI0LCAiZW1haWwiOiAiZi5uYXllYmlAZ21haWwuY29tIn0=</td><td>databricks_test</td><td>0</td><td>3</td><td>2020-01-20T18:57:15.946+0000</td><td>0</td></tr><tr><td>MQ==</td><td>eyJuYW1lIjogIkphbmUgRG9lIiwgImFnZSI6IDI5LCAiZW1haWwiOiAiamFuZS5kb2VAZ21haWwuY29tIn0=</td><td>databricks_test</td><td>0</td><td>4</td><td>2020-01-20T18:57:15.946+0000</td><td>0</td></tr><tr><td>Mw==</td><td>eyJuYW1lIjogIkpvaG4gRG9lIiwgImFnZSI6IDM1LCAiZW1haWwiOiAiam9obi5kb2VAZ21haWwuY29tIn0=</td><td>databricks_test</td><td>0</td><td>5</td><td>2020-01-20T18:57:16.004+0000</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## 2.2. Write a Kafka sink for batch queries"],"metadata":{}},{"cell_type":"markdown","source":["### 2.2.1. Create sample data"],"metadata":{}},{"cell_type":"code","source":["# import pyspark class Row from module sql\nfrom pyspark.sql import *"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Create the sample data\nsample_data = Row(\"key\", \"value\", \"topic\")\nsample1 = sample_data('1', '{\"name\": \"Jane Doe\", \"age\": 29, \"email\": \"jane.doe@gmail.com\"}', \"test-topic\")\nsample2 = sample_data('2', '{\"name\": \"Fatih\", \"age\": 24, \"email\": \"f.nayebi@gmail.com\"}', \"test-topic\")\nsample3 = sample_data('3', '{\"name\": \"John Doe\", \"age\": 35, \"email\": \"john.doe@gmail.com\"}', \"test-topic\")\n\nprint(sample1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Row(key=&#39;1&#39;, value=&#39;{&#34;name&#34;: &#34;Jane Doe&#34;, &#34;age&#34;: 29, &#34;email&#34;: &#34;jane.doe@gmail.com&#34;}&#39;, topic=&#39;test-topic&#39;)\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["### 2.2.2. Create a dataframe from sample data"],"metadata":{}},{"cell_type":"code","source":["df = spark.createDataFrame([sample1, sample2, sample3])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------------------+----------+\nkey|               value|     topic|\n+---+--------------------+----------+\n  1|{&#34;name&#34;: &#34;Jane Do...|test-topic|\n  2|{&#34;name&#34;: &#34;Fatih&#34;,...|test-topic|\n  3|{&#34;name&#34;: &#34;John Do...|test-topic|\n+---+--------------------+----------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th><th>topic</th></tr></thead><tbody><tr><td>1</td><td>{\"name\": \"Jane Doe\", \"age\": 29, \"email\": \"jane.doe@gmail.com\"}</td><td>test-topic</td></tr><tr><td>2</td><td>{\"name\": \"Fatih\", \"age\": 24, \"email\": \"f.nayebi@gmail.com\"}</td><td>test-topic</td></tr><tr><td>3</td><td>{\"name\": \"John Doe\", \"age\": 35, \"email\": \"john.doe@gmail.com\"}</td><td>test-topic</td></tr></tbody></table></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["### 2.2.3. Write data from a dataframe to a confluent kafka topic"],"metadata":{}},{"cell_type":"code","source":["# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\nds = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n    .write.format(\"kafka\")\\\n    .option(\"kafka.bootstrap.servers\", bootstrap.servers)\\\n    .option(\"subscribe\", topic_name)\\\n    .option(\"kafka.security.protocol\",\"SASL_SSL\")\\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"Replace with your api key\\\" password=\\\"replace with your api secret\\\";\")\\\n    .option(\"topic\", topic_name)\\\n    .save()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["# 3. Stream Processing"],"metadata":{}},{"cell_type":"markdown","source":["## 3.1. Read a stream from Kafka"],"metadata":{}},{"cell_type":"code","source":["df_stream = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", bootstrap.servers)\\\n    .option(\"subscribe\", topic_name)\\\n    .option(\"kafka.security.protocol\",\"SASL_SSL\")\\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"Replace with your api key\\\" password=\\\"replace with your api secret\\\";\")\\\n    .load() \\\n\ndisplay(df_stream)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th><th>topic</th><th>partition</th><th>offset</th><th>timestamp</th><th>timestampType</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["## 3.2. Write a Kafka sink for streaming queries"],"metadata":{}},{"cell_type":"code","source":["# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\nds = df_stream \\\n  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n  .writeStream \\\n  .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", bootstrap.servers)\\\n    .option(\"subscribe\", topic_name)\\\n    .option(\"kafka.security.protocol\",\"SASL_SSL\")\\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"Replace with your api key\\\" password=\\\"replace with your api secret\\\";\")\\\n  .option(\"topic\", \"databricks_test\") \\\n  .option(\"checkpointLocation\", \"/dbfs/dir\") \\\n  .start()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20}],"metadata":{"name":"Kafka-spark","notebookId":250246357806386},"nbformat":4,"nbformat_minor":0}
